{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from models import Model1, Model2\n",
    "from datasets import build_dataset1, build_dataset2\n",
    "from utils import pad_encode, emb_average, calc_acc\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "l, x, y = build_dataset1('/projets/melodi/gsantoss/data/yago/yago-class.nt',\n",
    "                         '/projets/melodi/gsantoss/data/yago/yago-schema.nt',\n",
    "                         '/projets/melodi/gsantoss/data/semclass1.pyo')\n",
    "print(y.shape)\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "x1 = tokenizer(x, return_tensors='pt', padding=True)\n",
    "x1_ids = x1['input_ids']\n",
    "x1_attention_mask = x1['attention_mask']\n",
    "vocab = set(itertools.chain(*(list(map(str.split, l)) + list(map(str.split, x)))))\n",
    "word_index = {q: (i + 1) for i, q in enumerate(vocab)}\n",
    "l2 = pad_encode(l, word_index)\n",
    "x2 = pad_encode(x, word_index)\n",
    "\n",
    "print(l2.shape, x2.shape)\n",
    "dataset = list(zip(x1_ids, x1_attention_mask, l2, x2, y))\n",
    "print(len(dataset))\n",
    "\n",
    "with open('/projets/melodi/gsantoss/embs/glove.6B.300d.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "glove = nn.Embedding(len(word_index) + 1, 300, padding_idx=0)\n",
    "glove2 = nn.Embedding(len(word_index) + 1, 300, padding_idx=0)\n",
    "glove.requires_grad_(False)\n",
    "glove2.requires_grad_(False)\n",
    "\n",
    "for l in tqdm(lines):\n",
    "    line = lines[0].split()\n",
    "    tk = line[0]\n",
    "    if tk not in word_index:\n",
    "        continue\n",
    "    emb = torch.Tensor(list(map(float, line[1:])))\n",
    "    glove.weight[word_index[tk]] = emb\n",
    "    glove2.weight[word_index[tk]] = emb"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/10 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dfadf1a38ebf4181ad1c8b6563efbcd9"
      },
      "application/json": {
       "n": 0,
       "total": 10,
       "elapsed": 0.014281988143920898,
       "ncols": null,
       "nrows": null,
       "prefix": "",
       "ascii": false,
       "unit": "it",
       "unit_scale": false,
       "rate": null,
       "bar_format": null,
       "postfix": null,
       "unit_divisor": 1000,
       "initial": 0,
       "colour": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=10)\n",
    "\n",
    "crit = nn.NLLLoss()\n",
    "\n",
    "data = []\n",
    "\n",
    "for fold, (train_i, test_i) in tqdm(enumerate(kf.split(dataset)), total=10):\n",
    "    train = [dataset[i] for i in train_i]\n",
    "    test = [dataset[i] for i in test_i]\n",
    "\n",
    "    x1i, x1a, x2l, x2x, y = zip(*train)\n",
    "    _, _, _, test_x2x, ty = zip(*test)\n",
    "\n",
    "    cy = torch.cat(list(map(lambda q: q.unsqueeze(0), y)))\n",
    "    cty = torch.cat(list(map(lambda q: q.unsqueeze(0), ty)))\n",
    "\n",
    "    train_em = emb_average(x2x, glove)\n",
    "    test_em = emb_average(test_x2x, glove)\n",
    "\n",
    "    y_pred = GaussianNB().fit(train_em, cy).predict(test_em)\n",
    "    data.append([fold, 'GaussianBN', calc_acc(y_pred, cty)])\n",
    "    y_pred = make_pipeline(StandardScaler(), SVC(gamma='auto')).fit(train_em, cy).predict(test_em)\n",
    "    data.append([fold, 'SVM', calc_acc(y_pred, cty)])\n",
    "    y_pred = DecisionTreeClassifier().fit(train_em, cy).predict(test_em)\n",
    "    data.append([fold, 'DecisionTree', calc_acc(y_pred, cty)])\n",
    "    y_pred = RandomForestClassifier().fit(train_em, cy).predict(test_em)\n",
    "    data.append([fold, 'RandomForest', calc_acc(y_pred, cty)])\n",
    "\n",
    "    model1 = Model1(6)\n",
    "    model1.cuda(0)\n",
    "    model2 = Model2(glove2, len(word_index), 6)\n",
    "    model2.cuda(1)\n",
    "    optimizer1 = optim.Adam(model1.parameters(), lr=0.00003)\n",
    "    optimizer2 = optim.Adam(model2.parameters(), lr=0.003)\n",
    "\n",
    "    for x1i, x1a, x2l, x2x, y in DataLoader(train, batch_size=32, shuffle=True):\n",
    "        optimizer1.zero_grad()\n",
    "        optimizer2.zero_grad()\n",
    "\n",
    "        out1 = model1(x1i.cuda(0), x1a.cuda(0)).cpu()\n",
    "        out2 = model2(x2l.cuda(1), x2x.cuda(1)).cpu()\n",
    "\n",
    "        l1 = crit(out1, y)\n",
    "        l1.backward()\n",
    "\n",
    "        l2 = crit(out2, y)\n",
    "        l2.backward()\n",
    "\n",
    "        optimizer1.step()\n",
    "        optimizer2.step()\n",
    "\n",
    "    model1.eval()\n",
    "    model2.eval()\n",
    "\n",
    "    out1 = []\n",
    "    out2 = []\n",
    "    with torch.no_grad():\n",
    "        for x1i, x1a, x2l, x2x, y in DataLoader(test, batch_size=32, shuffle=True):\n",
    "            o1 = model1(x1i.cuda(0), x1a.cuda(0)).exp().cpu()\n",
    "            out1.append(o1.argmax(dim=1) == y)\n",
    "            o2 = model2(x2l.cuda(1), x2x.cuda(1)).exp().cpu()\n",
    "            out2.append(o2.argmax(dim=1) == y)\n",
    "\n",
    "    out1 = torch.cat(out1).float()\n",
    "    acc1 = out1.sum() / out1.shape[0]\n",
    "    data.append([fold, 'Model1', acc1.item()])\n",
    "    out2 = torch.cat(out2).float()\n",
    "    acc2 = out2.sum() / out2.shape[0]\n",
    "    data.append([fold, 'Model2', acc2.item()])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Acc\n",
      "Name                  \n",
      "DecisionTree  0.255359\n",
      "GaussianBN    0.266151\n",
      "RandomForest  0.399599\n",
      "SVM           0.537388\n",
      "Model2        0.583894\n",
      "Model1        0.842978\n"
     ]
    }
   ],
   "source": [
    "dt = pd.DataFrame(data, columns=['Fold', 'Name', 'Acc'])\n",
    "\n",
    "res = dt.groupby('Name').mean().sort_values('Acc')\n",
    "\n",
    "print(res.drop(columns=['Fold']))\n",
    "res.to_csv('exp1_data1.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "c, x, y = build_dataset2('/projets/melodi/gsantoss/data/dataset2.csv')\n",
    "x1 = tokenizer(list(x), return_tensors='pt', padding=True)\n",
    "x1_ids = x1['input_ids']\n",
    "x1_attention_mask = x1['attention_mask']\n",
    "print(x1_ids.shape)\n",
    "vocab = set(itertools.chain(*(list(map(str.split, c)) + list(map(str.split, x)))))\n",
    "\n",
    "word_index = {q: (i + 1) for i, q in enumerate(vocab)}\n",
    "print(len(word_index))\n",
    "l2 = pad_encode(c, word_index)\n",
    "x2 = pad_encode(x, word_index)\n",
    "\n",
    "print(l2.shape, x2.shape)\n",
    "dataset = list(zip(x1_ids, x1_attention_mask, l2, x2, y))\n",
    "print(len(dataset))\n",
    "\n",
    "glove = nn.Embedding(len(word_index) + 1, 300, padding_idx=0)\n",
    "glove2 = nn.Embedding(len(word_index) + 1, 300, padding_idx=0)\n",
    "glove.requires_grad_(False)\n",
    "glove2.requires_grad_(False)\n",
    "\n",
    "for l in tqdm(lines):\n",
    "    line = lines[0].split()\n",
    "    tk = line[0]\n",
    "    if tk not in word_index:\n",
    "        continue\n",
    "    emb = torch.Tensor(list(map(float, line[1:])))\n",
    "    glove.weight[word_index[tk]] = emb\n",
    "    glove2.weight[word_index[tk]] = emb"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/10 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9191337afbb74d02aa85bf51be456b6f"
      },
      "application/json": {
       "n": 0,
       "total": 10,
       "elapsed": 0.016420841217041016,
       "ncols": null,
       "nrows": null,
       "prefix": "",
       "ascii": false,
       "unit": "it",
       "unit_scale": false,
       "rate": null,
       "bar_format": null,
       "postfix": null,
       "unit_divisor": 1000,
       "initial": 0,
       "colour": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=10)\n",
    "\n",
    "crit = nn.NLLLoss()\n",
    "\n",
    "data = []\n",
    "\n",
    "for fold, (train_i, test_i) in tqdm(enumerate(kf.split(dataset)), total=10):\n",
    "    train = [dataset[i] for i in train_i]\n",
    "    test = [dataset[i] for i in test_i]\n",
    "\n",
    "    x1i, x1a, x2l, x2x, y = zip(*train)\n",
    "    _, _, _, test_x2x, ty = zip(*test)\n",
    "\n",
    "    cy = torch.cat(list(map(lambda q: q.unsqueeze(0), y)))\n",
    "    cty = torch.cat(list(map(lambda q: q.unsqueeze(0), ty)))\n",
    "\n",
    "    train_em = emb_average(x2x, glove)\n",
    "    test_em = emb_average(test_x2x, glove)\n",
    "\n",
    "    y_pred = GaussianNB().fit(train_em, cy).predict(test_em)\n",
    "    data.append([fold, 'GaussianBN', calc_acc(y_pred, cty)])\n",
    "    y_pred = make_pipeline(StandardScaler(), SVC(gamma='auto')).fit(train_em, cy).predict(test_em)\n",
    "    data.append([fold, 'SVM', calc_acc(y_pred, cty)])\n",
    "    y_pred = DecisionTreeClassifier().fit(train_em, cy).predict(test_em)\n",
    "    data.append([fold, 'DecisionTree', calc_acc(y_pred, cty)])\n",
    "    y_pred = RandomForestClassifier().fit(train_em, cy).predict(test_em)\n",
    "    data.append([fold, 'RandomForest', calc_acc(y_pred, cty)])\n",
    "\n",
    "    model1 = Model1(5)\n",
    "    model1.cuda(0)\n",
    "    model2 = Model2(glove2, len(word_index), 5)\n",
    "    model2.cuda(1)\n",
    "    optimizer1 = optim.Adam(model1.parameters(), lr=0.00003)\n",
    "    optimizer2 = optim.Adam(model2.parameters(), lr=0.003)\n",
    "\n",
    "    for x1i, x1a, x2l, x2x, y in DataLoader(train, batch_size=32, shuffle=True):\n",
    "        optimizer1.zero_grad()\n",
    "        optimizer2.zero_grad()\n",
    "\n",
    "        out1 = model1(x1i.cuda(0), x1a.cuda(0)).cpu()\n",
    "        out2 = model2(x2l.cuda(1), x2x.cuda(1)).cpu()\n",
    "\n",
    "        l1 = crit(out1, y)\n",
    "        l1.backward()\n",
    "\n",
    "        l2 = crit(out2, y)\n",
    "        l2.backward()\n",
    "\n",
    "        optimizer1.step()\n",
    "        optimizer2.step()\n",
    "\n",
    "    model1.eval()\n",
    "    model2.eval()\n",
    "\n",
    "    out1 = []\n",
    "    out2 = []\n",
    "    with torch.no_grad():\n",
    "        for x1i, x1a, x2l, x2x, y in DataLoader(test, batch_size=32, shuffle=True):\n",
    "            o1 = model1(x1i.cuda(0), x1a.cuda(0)).exp().cpu()\n",
    "            out1.append(o1.argmax(dim=1) == y)\n",
    "            o2 = model2(x2l.cuda(1), x2x.cuda(1)).exp().cpu()\n",
    "            out2.append(o2.argmax(dim=1) == y)\n",
    "\n",
    "    out1 = torch.cat(out1).float()\n",
    "    acc1 = out1.sum() / out1.shape[0]\n",
    "    data.append([fold, 'Model1', acc1.item()])\n",
    "    out2 = torch.cat(out2).float()\n",
    "    acc2 = out2.sum() / out2.shape[0]\n",
    "    data.append([fold, 'Model2', acc2.item()])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Acc\n",
      "Name                  \n",
      "SVM           0.219325\n",
      "GaussianBN    0.238072\n",
      "RandomForest  0.263596\n",
      "DecisionTree  0.342751\n",
      "Model2        0.378217\n",
      "Model1        0.559342\n"
     ]
    }
   ],
   "source": [
    "dt = pd.DataFrame(data, columns=['Fold', 'Name', 'Acc'])\n",
    "\n",
    "res = dt.groupby('Name').mean().sort_values('Acc')\n",
    "\n",
    "print(res.drop(columns=['Fold']))\n",
    "res.to_csv('exp2_data2.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "myenv",
   "language": "python",
   "display_name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
